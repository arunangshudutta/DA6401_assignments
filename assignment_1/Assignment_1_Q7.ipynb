{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOhuMKNRmUzDQCxXvsW46At",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arunangshudutta/DA6401_assignments/blob/main/assignment_1/Assignment_1_Q7.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "z8_X4yFu9cYo"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore', category=RuntimeWarning)\n",
        "\n",
        "from keras.datasets import fashion_mnist\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import wandb"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Functions"
      ],
      "metadata": {
        "id": "6S9wUUyy98UB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def weights_initialization(num_neurons, initializer):\n",
        "  \"\"\"\n",
        "  num_neurons = list of number of neurons at each layer starting from the input layer and ending at output layer\n",
        "  initializer = 'random' or 'xavier'\n",
        "\n",
        "  Returns: initialized weight matrices and bias vectors\n",
        "  \"\"\"\n",
        "  mean=0\n",
        "  std_dev=1\n",
        "\n",
        "  W_matrices = []\n",
        "  b_vectors = []\n",
        "\n",
        "  for i in range(len(num_neurons)-1):\n",
        "    rows = num_neurons[i+1]\n",
        "    cols = num_neurons[i]\n",
        "\n",
        "    if initializer == 'random':\n",
        "\n",
        "      weight_matrix = np.random.normal(mean, std_dev, size=(rows, cols))\n",
        "\n",
        "    elif initializer == 'Xavier':\n",
        "\n",
        "      upper_bound = np.sqrt(6.0/(rows + cols))\n",
        "      lower_bound = -1*upper_bound\n",
        "      weight_matrix = np.random.uniform(low = lower_bound, high = upper_bound, size = (rows, cols))\n",
        "\n",
        "    else:\n",
        "      print('initializer invalid')\n",
        "\n",
        "\n",
        "    bias_vector = np.zeros((rows,1))\n",
        "\n",
        "    W_matrices.append(weight_matrix)\n",
        "    b_vectors.append(bias_vector)\n",
        "\n",
        "\n",
        "  return W_matrices, b_vectors\n",
        "\n",
        "########################################## ACTIVATION FUNCTIONS\n",
        "def relu(x):\n",
        "  \"\"\"\n",
        "  Rectified Linear Unit (ReLU) activation function\n",
        "  \"\"\"\n",
        "  return np.maximum(0, x)\n",
        "\n",
        "def sigmoid(x):\n",
        "  \"\"\"\n",
        "  Sigmoid activation function\n",
        "  \"\"\"\n",
        "  # x = np.float128(x)\n",
        "  return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def tanh(x):\n",
        "  \"\"\"\n",
        "  Hyperbolic tangent (tanh) activation function\n",
        "  \"\"\"\n",
        "  # x = np.float128(x)\n",
        "  return np.tanh(x)\n",
        "def softmax(x):\n",
        "\n",
        "  \"\"\"\n",
        "  Softmax function for output layer\n",
        "  \"\"\"\n",
        "  # x = np.float128(x)\n",
        "  return np.exp(x) / np.sum(np.exp(x), axis=0)\n",
        "\n",
        "def activation_output(x, activation_function):\n",
        "  \"\"\"\n",
        "  activation_function = 'ReLU', 'sigmoid', 'tanh'\n",
        "  \"\"\"\n",
        "  if activation_function == 'ReLU':\n",
        "    return relu(x)\n",
        "  elif activation_function == 'sigmoid':\n",
        "    return sigmoid(x)\n",
        "  elif activation_function == 'tanh':\n",
        "    return tanh(x)\n",
        "  elif activation_function == 'softmax':\n",
        "    return softmax(x)\n",
        "  else:\n",
        "    print('activation function invalid')\n",
        "\n",
        "######################################### DERIVATIVE OF ACTIVATION FUNCTION\n",
        "def sigmoid_derivative(x):\n",
        "  s = sigmoid(x)\n",
        "  return s * (1 - s)\n",
        "\n",
        "def tanh_derivative(x):\n",
        "  t = tanh(x)\n",
        "  return 1 - t**2\n",
        "\n",
        "def relu_derivative(x):\n",
        "  return 1*(x>0)\n",
        "\n",
        "def activation_derivative(x, activation_function):\n",
        "  \"\"\"\n",
        "  activation_function = 'ReLU', 'sigmoid', 'tanh'\n",
        "  \"\"\"\n",
        "  if activation_function == 'ReLU':\n",
        "    return relu_derivative(x)\n",
        "  elif activation_function == 'sigmoid':\n",
        "    return sigmoid_derivative(x)\n",
        "  elif activation_function == 'tanh':\n",
        "    return tanh_derivative(x)\n",
        "  else:\n",
        "    print('activation function invalid')\n",
        "\n",
        "####################################### Forward Propagation\n",
        "\n",
        "def layer_output_FP(x, weight_matrix, bias_vector, activation_function):\n",
        "  pre_activation = np.add(np.matmul(weight_matrix, x), bias_vector)\n",
        "  post_activation = activation_output(pre_activation, activation_function)\n",
        "  return pre_activation, post_activation\n",
        "\n",
        "def forward_propagation(ip_data, W_matrices, b_vectors, activation_functions):\n",
        "  \"\"\"\n",
        "  forward propagation\n",
        "  \"\"\"\n",
        "\n",
        "  layer_op = []\n",
        "  layer_op.append(ip_data)\n",
        "\n",
        "  layer_ip = []\n",
        "\n",
        "  for i in range(len(W_matrices)):\n",
        "\n",
        "    weight_matrix = W_matrices[i]\n",
        "    bias_vector = b_vectors[i]\n",
        "\n",
        "    activation_function = activation_functions[i]\n",
        "\n",
        "    pre_activation, post_activation = layer_output_FP(layer_op[i], weight_matrix, bias_vector, activation_function)\n",
        "\n",
        "    layer_op.append(post_activation)\n",
        "    layer_ip.append(pre_activation)\n",
        "\n",
        "  return layer_ip, layer_op\n",
        "\n",
        "####################################### Back Propagation\n",
        "\n",
        "def back_propagation(W_matrices, b_vectors, y_true, layer_ip, layer_op, activation_functions, batch_size, w_d):\n",
        "\n",
        "  DWs = []\n",
        "  Dbs = []\n",
        "  for i in range(len(W_matrices)):\n",
        "    k = len(W_matrices) - i\n",
        "\n",
        "\n",
        "    if k == len(W_matrices):\n",
        "      Da = -np.add(y_true, -layer_op[k])\n",
        "      Dw = (np.matmul(Da, layer_op[k-1].T) + w_d*W_matrices[k-1])/batch_size\n",
        "    else:\n",
        "\n",
        "      Dh = np.matmul(W_matrices[k].T, Da)\n",
        "      Dg = activation_derivative(layer_ip[k-1], activation_functions[k-1])\n",
        "      Da = np.multiply(Dh, Dg)\n",
        "      Dw = (np.matmul(Da, layer_op[k-1].T) + w_d*W_matrices[k-1])/batch_size\n",
        "    Db = np.sum(Da, axis=1, keepdims=True)/batch_size\n",
        "\n",
        "    DWs.append(Dw)\n",
        "    Dbs.append(Db)\n",
        "\n",
        "  return DWs, Dbs\n",
        "\n",
        "\n",
        "################################## optimization functions\n",
        "\n",
        "def update_weights_gd(W_matrices, b_vectors, DWs, Dbs, learning_rate = 0.1):\n",
        "  DWs.reverse()\n",
        "  Dbs.reverse()\n",
        "\n",
        "  for i in range(len(DWs)):\n",
        "\n",
        "    W_matrices[i] = W_matrices[i] - learning_rate*DWs[i]\n",
        "    b_vectors[i] = b_vectors[i] - learning_rate*Dbs[i]\n",
        "  return W_matrices, b_vectors\n",
        "\n",
        "def update_weights_momentum(W_matrices, b_vectors, DWs, Dbs, u_past_w, u_past_b, learning_rate = 0.1, beta = 0.5):\n",
        "  DWs.reverse()\n",
        "  Dbs.reverse()\n",
        "  u_w = u_past_w\n",
        "  u_b = u_past_b\n",
        "  for i in range(len(DWs)):\n",
        "\n",
        "    u_w[i] = beta*u_past_w[i] + DWs[i]\n",
        "    u_b[i] = beta*u_past_b[i] + Dbs[i]\n",
        "\n",
        "    W_matrices[i] = W_matrices[i] - learning_rate*u_w[i]\n",
        "    b_vectors[i] = b_vectors[i] - learning_rate*u_b[i]\n",
        "\n",
        "  return W_matrices, b_vectors, u_w, u_b\n",
        "\n",
        "def update_weights_adagrad(W_matrices, b_vectors, DWs, Dbs, u_past_w, u_past_b, learning_rate = 0.1):\n",
        "  DWs.reverse()\n",
        "  Dbs.reverse()\n",
        "\n",
        "  u_w = u_past_w\n",
        "  u_b = u_past_b\n",
        "  eps = 1e-8\n",
        "  for i in range(len(DWs)):\n",
        "    u_w[i] = u_past_w[i] + DWs[i]**2\n",
        "    u_b[i] = u_past_b[i] + Dbs[i]**2\n",
        "\n",
        "    W_matrices[i] = W_matrices[i] - learning_rate*DWs[i]/(np.sqrt(u_w[i]) + eps)\n",
        "    b_vectors[i] = b_vectors[i] - learning_rate*Dbs[i]/(np.sqrt(u_b[i]) + eps)\n",
        "\n",
        "  return W_matrices, b_vectors, u_w, u_b\n",
        "\n",
        "def update_weights_rmsprop(W_matrices, b_vectors, DWs, Dbs, u_past_w, u_past_b, learning_rate = 0.1, beta = 0.5):\n",
        "  DWs.reverse()\n",
        "  Dbs.reverse()\n",
        "\n",
        "  u_w = u_past_w\n",
        "  u_b = u_past_b\n",
        "  eps = 1e-8\n",
        "  for i in range(len(DWs)):\n",
        "    u_w[i] = beta*u_past_w[i] + (1-beta)*DWs[i]**2\n",
        "    u_b[i] = beta*u_past_b[i] + (1-beta)*Dbs[i]**2\n",
        "\n",
        "    W_matrices[i] = W_matrices[i] - learning_rate*DWs[i]/(np.sqrt(u_w[i]) + eps)\n",
        "    b_vectors[i] = b_vectors[i] - learning_rate*Dbs[i]/(np.sqrt(u_b[i]) + eps)\n",
        "\n",
        "  return W_matrices, b_vectors, u_w, u_b\n",
        "\n",
        "def update_weights_adam(W_matrices, b_vectors, DWs, Dbs, mw_past, mb_past, vw_past, vb_past, t, learning_rate = 0.1, beta1 = 0.5, beta2 =0.5):\n",
        "  DWs.reverse()\n",
        "  Dbs.reverse()\n",
        "  mw = mw_past\n",
        "  mb = mb_past\n",
        "  vw = vw_past\n",
        "  vb = vb_past\n",
        "  eps = 1e-8\n",
        "\n",
        "  for i in range(len(DWs)):\n",
        "    mw[i] = beta1*mw_past[i] + (1-beta1)*DWs[i]\n",
        "    mb[i] = beta1*mb_past[i] + (1-beta1)*Dbs[i]\n",
        "\n",
        "    mw_cap = mw[i]/(1 - beta1**t)\n",
        "    mb_cap = mb[i]/(1 - beta1**t)\n",
        "\n",
        "    vw[i] = beta2*vw_past[i] + (1-beta2)*DWs[i]**2\n",
        "    vb[i] = beta2*vb_past[i] + (1-beta2)*Dbs[i]**2\n",
        "    vw_cap = vw[i]/(1 - beta2**t)\n",
        "    vb_cap = vb[i]/(1 - beta2**t)\n",
        "\n",
        "    W_matrices[i] = W_matrices[i] - learning_rate*mw_cap/(np.sqrt(vw_cap) + eps)\n",
        "    b_vectors[i] = b_vectors[i] - learning_rate*mb_cap/(np.sqrt(vb_cap) + eps)\n",
        "\n",
        "  return W_matrices, b_vectors, mw, mb, vw, vb\n",
        "\n",
        "def update_weights_nadam(W_matrices, b_vectors, DWs, Dbs, mw_past, mb_past, vw_past, vb_past,t,  learning_rate = 0.1, beta1 = 0.5, beta2 =0.5):\n",
        "  DWs.reverse()\n",
        "  Dbs.reverse()\n",
        "  mw = mw_past\n",
        "  mb = mb_past\n",
        "  vw = vw_past\n",
        "  vb = vb_past\n",
        "  eps = 1e-8\n",
        "\n",
        "  for i in range(len(DWs)):\n",
        "    mw[i] = beta1*mw_past[i] + (1-beta1)*DWs[i]\n",
        "    mb[i] = beta1*mb_past[i] + (1-beta1)*Dbs[i]\n",
        "\n",
        "    mw_cap = mw[i]/(1 - beta1**(t+1))\n",
        "    mb_cap = mb[i]/(1 - beta1**(t+1))\n",
        "\n",
        "    vw[i] = beta2*vw_past[i] + (1-beta2)*DWs[i]**2\n",
        "    vb[i] = beta2*vb_past[i] + (1-beta2)*Dbs[i]**2\n",
        "    vw_cap = vw[i]/(1 - beta2**(t+1))\n",
        "    vb_cap = vb[i]/(1 - beta2**(t+1))\n",
        "\n",
        "    W_matrices[i] = W_matrices[i] - learning_rate*(beta1*mw_cap + ((1-beta1)/(1 - beta1**(t+1)))*DWs[i])/(np.sqrt(vw_cap) + eps)\n",
        "    b_vectors[i] = b_vectors[i] - learning_rate*(beta1*mb_cap + ((1-beta1)/(1 - beta1**(t+1)))*Dbs[i])/(np.sqrt(vb_cap) + eps)\n",
        "\n",
        "  return W_matrices, b_vectors, mw, mb, vw, vb\n",
        "\n",
        "def look_ahead_nag(W_s, b_s, u_past_w, u_past_b, beta = 0.5):\n",
        "  for i in range(len(W_s)):\n",
        "    W_s[i] = W_s[i] - beta*u_past_w[i]\n",
        "    b_s[i] = b_s[i] - beta*u_past_b[i]\n",
        "  return W_s, b_s\n",
        "\n",
        "############################# Loss and accuracy\n",
        "\n",
        "def one_hot_encode(integers, num_classes=None):\n",
        "  if num_classes is None:\n",
        "      num_classes = np.max(integers) + 1\n",
        "  return np.eye(num_classes)[integers]\n",
        "\n",
        "def cross_entropy_loss(y_true, y_pred, batch_size):\n",
        "  # Clip the predicted probabilities to avoid numerical instability\n",
        "  y_pred = np.clip(y_pred, 1e-15, 1 - 1e-15)\n",
        "  loss_value = np.sum(np.sum(y_true*np.log(y_pred), axis=0))/batch_size\n",
        "  return loss_value*(-1)\n",
        "\n",
        "\n",
        "\n",
        "def accuracy(y_true, y_pred, batch_size):\n",
        "  n_correct = 0\n",
        "  for i in range(0, batch_size, 1) :\n",
        "    if y_true[:,i].argmax() == y_pred[:,i].argmax() :\n",
        "      n_correct += 1\n",
        "  return 100 * n_correct / batch_size\n",
        "\n",
        "###################################### dataset\n",
        "\n",
        "def load_split_dataset():\n",
        "  # Load Fashion MNIST dataset\n",
        "  (train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n",
        "\n",
        "  # Split the training set into training and validation sets\n",
        "  X_train, X_val, Y_train, Y_val = train_test_split(train_images, train_labels, test_size=0.3, random_state=42)\n",
        "\n",
        "  data_size = X_train.shape[0]\n",
        "  X_train = (X_train.reshape(data_size, -1).T)/255\n",
        "  Y_train = one_hot_encode(Y_train, 10).T\n",
        "\n",
        "  data_size = X_val.shape[0]\n",
        "  X_val = (X_val.reshape(data_size, -1).T)/255\n",
        "  Y_val = one_hot_encode(Y_val, 10).T\n",
        "\n",
        "  data_size = test_images.shape[0]\n",
        "  X_test = (test_images.reshape(data_size, -1).T)/255\n",
        "  Y_test = one_hot_encode(test_labels, 10).T\n",
        "\n",
        "  return X_train, Y_train, X_val, Y_val, X_test, Y_test"
      ],
      "metadata": {
        "id": "4P2lyNXw98vO"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train function"
      ],
      "metadata": {
        "id": "2nNpcxWV9_G0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "def train_model_q7(X_train,Y_train, X_test, Y_test, epoch=1,batch_size=25, num_neurons_hidden = [10], activation_functions = ['sigmoid'],\n",
        "                weights_init_type='random', optimizer = 'sgd', learning_rate = 0.1, opti_beta = [0.5, 0.5], w_d = 0):\n",
        "\n",
        "  \"\"\"\n",
        "  X has shape (number of features, number of samples in train data set)\n",
        "  Y has shape (number of classes, number of samples in train data set)\n",
        "\n",
        "  num_neurons_hidden = list of number of neurons at each hidden layer\n",
        "\n",
        "  \"\"\"\n",
        "  num_ip_neurons = X_train.shape[0]\n",
        "  num_op_neurons = Y_train.shape[0]\n",
        "  num_neurons = [num_ip_neurons] + num_neurons_hidden + [num_op_neurons]\n",
        "  activation_functions = activation_functions + ['softmax']\n",
        "\n",
        "  W_s, b_s = weights_initialization(num_neurons, weights_init_type)\n",
        "\n",
        "  print(\"Architecture Description:\\n\")\n",
        "  for i in range(len(num_neurons)-1):\n",
        "    print(\"Layer: \", i+1, \" ; number of neurons: \", num_neurons[i+1], \" ; activation function: \", activation_functions[i])\n",
        "    print(\"Weight matrix dimention\", W_s[i].shape, \"Bias vector dimention\", b_s[i].shape)\n",
        "    print(\"----------------\")\n",
        "\n",
        "  num_batches = np.floor(X_train.shape[1]/batch_size)\n",
        "  print(num_batches)\n",
        "\n",
        "  print('\\n Start of training')\n",
        "\n",
        "  if optimizer == 'momentum':\n",
        "    u_past_w = [x * 0 for x in W_s]\n",
        "    u_past_b = [x * 0 for x in b_s]\n",
        "\n",
        "  elif optimizer == 'nag':\n",
        "    u_past_w = [x * 0 for x in W_s]\n",
        "    u_past_b = [x * 0 for x in b_s]\n",
        "\n",
        "  elif optimizer == 'rmsprop':\n",
        "    u_past_w = [x * 0 for x in W_s]\n",
        "    u_past_b = [x * 0 for x in b_s]\n",
        "\n",
        "  elif optimizer == 'adagrad':\n",
        "    u_past_w = [x * 0 for x in W_s]\n",
        "    u_past_b = [x * 0 for x in b_s]\n",
        "\n",
        "  elif optimizer == 'adam':\n",
        "    mw_past = [x * 0 for x in W_s]\n",
        "    mb_past = [x * 0 for x in b_s]\n",
        "    vw_past = [x * 0 for x in W_s]\n",
        "    vb_past = [x * 0 for x in b_s]\n",
        "    t = 1\n",
        "\n",
        "  elif optimizer == 'nadam':\n",
        "    mw_past = [x * 0 for x in W_s]\n",
        "    mb_past = [x * 0 for x in b_s]\n",
        "    vw_past = [x * 0 for x in W_s]\n",
        "    vb_past = [x * 0 for x in b_s]\n",
        "    t = 1\n",
        "\n",
        "  ip_all, op_all = forward_propagation(X_train, W_s, b_s, activation_functions)\n",
        "  loss_tr = cross_entropy_loss(Y_train, op_all[-1], X_train.shape[1])\n",
        "  acc = accuracy(Y_train, op_all[-1], batch_size)\n",
        "  print(\"Training Loss: \", loss_tr)\n",
        "  print(\"Training Accuracy: \", acc)\n",
        "\n",
        "  ip_all, op_all = forward_propagation(X_test, W_s, b_s, activation_functions)\n",
        "  loss_ts = cross_entropy_loss(Y_test, op_all[-1], X_test.shape[1])\n",
        "  acc = accuracy(Y_test, op_all[-1], batch_size)\n",
        "  print(\"Test Loss: \", loss_ts)\n",
        "  print(\"Testing Accuracy: \", acc)\n",
        "\n",
        "  for i in range(epoch):\n",
        "    print('Epoch: ', i)\n",
        "\n",
        "    for j in tqdm(range(int(num_batches))):\n",
        "      batch_X = X_train[:,j*batch_size:(j+1)*batch_size]\n",
        "      batch_Y = Y_train[:,j*batch_size:(j+1)*batch_size]\n",
        "\n",
        "\n",
        "      if optimizer == 'sgd':\n",
        "        ip, op = forward_propagation(batch_X, W_s, b_s, activation_functions)\n",
        "        DWs, Dbs = back_propagation(W_s, b_s, batch_Y, ip, op, activation_functions, batch_size, w_d)\n",
        "        W_s, b_s = update_weights_gd(W_s, b_s, DWs, Dbs, learning_rate)\n",
        "\n",
        "      elif optimizer == 'momentum':\n",
        "        ip, op = forward_propagation(batch_X, W_s, b_s, activation_functions)\n",
        "        DWs, Dbs = back_propagation(W_s, b_s, batch_Y, ip, op, activation_functions, batch_size, w_d)\n",
        "        W_s, b_s, u_past_w, u_past_b  = update_weights_momentum(W_s, b_s, DWs, Dbs, u_past_w, u_past_b, learning_rate, opti_beta[0])\n",
        "\n",
        "      elif optimizer == 'adagrad':\n",
        "        ip, op = forward_propagation(batch_X, W_s, b_s, activation_functions)\n",
        "        DWs, Dbs = back_propagation(W_s, b_s, batch_Y, ip, op, activation_functions, batch_size, w_d)\n",
        "        W_s, b_s, u_past_w, u_past_b  = update_weights_adagrad(W_s, b_s, DWs, Dbs, u_past_w, u_past_b, learning_rate)\n",
        "\n",
        "      elif optimizer == 'rmsprop':\n",
        "        ip, op = forward_propagation(batch_X, W_s, b_s, activation_functions)\n",
        "        DWs, Dbs = back_propagation(W_s, b_s, batch_Y, ip, op, activation_functions, batch_size, w_d)\n",
        "        W_s, b_s, u_past_w, u_past_b  = update_weights_rmsprop(W_s, b_s, DWs, Dbs, u_past_w, u_past_b, learning_rate, opti_beta[0])\n",
        "\n",
        "      elif optimizer == 'adam':\n",
        "        ip, op = forward_propagation(batch_X, W_s, b_s, activation_functions)\n",
        "        DWs, Dbs = back_propagation(W_s, b_s, batch_Y, ip, op, activation_functions, batch_size, w_d)\n",
        "        W_s, b_s, mw_past, mb_past, vw_past, vb_past = update_weights_adam(W_s, b_s, DWs, Dbs, mw_past, mb_past, vw_past, vb_past, t, learning_rate, opti_beta[0], opti_beta[1])\n",
        "        t =t +1\n",
        "\n",
        "      elif optimizer == 'nadam':\n",
        "        ip, op = forward_propagation(batch_X, W_s, b_s, activation_functions)\n",
        "        DWs, Dbs = back_propagation(W_s, b_s, batch_Y, ip, op, activation_functions, batch_size, w_d)\n",
        "        W_s, b_s, mw_past, mb_past, vw_past, vb_past = update_weights_nadam(W_s, b_s, DWs, Dbs, mw_past, mb_past, vw_past, vb_past, t, learning_rate, opti_beta[0], opti_beta[1])\n",
        "        t =t +1\n",
        "      elif optimizer == 'nag':\n",
        "        PWs, Pbs = look_ahead_nag(W_s, b_s, u_past_w, u_past_b, opti_beta[0])\n",
        "        ip, op = forward_propagation(batch_X, PWs, Pbs, activation_functions)\n",
        "        DWs, Dbs = back_propagation(PWs, Pbs, batch_Y, ip, op, activation_functions, batch_size, w_d)\n",
        "        W_s, b_s, u_past_w, u_past_b  = update_weights_momentum(W_s, b_s, DWs, Dbs, u_past_w, u_past_b, learning_rate, opti_beta[0])\n",
        "\n",
        "\n",
        "    ip_all, op_all = forward_propagation(X_train, W_s, b_s, activation_functions)\n",
        "    loss_tr = cross_entropy_loss(Y_train, op_all[-1], X_train.shape[1])\n",
        "    acc = accuracy(Y_train, op_all[-1], batch_size)\n",
        "    print(\"Training Loss: \", loss_tr)\n",
        "    print(\"Training Accuracy: \", acc)\n",
        "\n",
        "    ip_all, op_all = forward_propagation(X_test, W_s, b_s, activation_functions)\n",
        "    loss_ts = cross_entropy_loss(Y_test, op_all[-1], X_test.shape[1])\n",
        "    acc = accuracy(Y_test, op_all[-1], batch_size)\n",
        "    print(\"Test Loss: \", loss_ts)\n",
        "    print(\"Testing Accuracy: \", acc)\n",
        "\n",
        "  ip_all, op_all = forward_propagation(X_test, W_s, b_s, activation_functions)\n",
        "\n",
        "  return op_all[-1]\n"
      ],
      "metadata": {
        "id": "F6YXq2-a-CMT"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, Y_train, X_val, Y_val, X_test, Y_test = load_split_dataset()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KPT972SD-UyK",
        "outputId": "0cf81b99-8c05-447a-c877-5eefc39c75c7"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "\u001b[1m29515/29515\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "\u001b[1m26421880/26421880\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "\u001b[1m5148/5148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "\u001b[1m4422102/4422102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
        "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
        "\n",
        "epochs = 10\n",
        "nhl = 3\n",
        "sz = 64\n",
        "w_d = 0\n",
        "lr = 0.0001\n",
        "optimizer = 'adam'\n",
        "b_sz = 16\n",
        "weight_init = 'Xavier'\n",
        "act_fun = 'tanh'\n",
        "\n",
        "neuros_num = []\n",
        "act_func = []\n",
        "for i in range(nhl):\n",
        "  neuros_num.append(sz)\n",
        "  act_func.append(act_fun)\n",
        "\n",
        "Y_pred = train_model_q7(X_train, Y_train, X_test, Y_test, epoch=epochs, batch_size=b_sz, num_neurons_hidden = neuros_num, activation_functions = act_func,\n",
        "                weights_init_type=weight_init, optimizer = optimizer, learning_rate = lr, opti_beta = [0.5, 0.5], w_d = w_d)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ub9FyGyD-XuL",
        "outputId": "aed87d8d-0307-4c41-d4d7-3cdfedc855e5"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Architecture Description:\n",
            "\n",
            "Layer:  1  ; number of neurons:  64  ; activation function:  tanh\n",
            "Weight matrix dimention (64, 784) Bias vector dimention (64, 1)\n",
            "----------------\n",
            "Layer:  2  ; number of neurons:  64  ; activation function:  tanh\n",
            "Weight matrix dimention (64, 64) Bias vector dimention (64, 1)\n",
            "----------------\n",
            "Layer:  3  ; number of neurons:  64  ; activation function:  tanh\n",
            "Weight matrix dimention (64, 64) Bias vector dimention (64, 1)\n",
            "----------------\n",
            "Layer:  4  ; number of neurons:  10  ; activation function:  softmax\n",
            "Weight matrix dimention (10, 64) Bias vector dimention (10, 1)\n",
            "----------------\n",
            "2625.0\n",
            "\n",
            " Start of training\n",
            "Training Loss:  2.379642495449149\n",
            "Training Accuracy:  12.5\n",
            "Test Loss:  2.379694307797713\n",
            "Testing Accuracy:  12.5\n",
            "Epoch:  0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2625/2625 [00:07<00:00, 366.23it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss:  0.480910506395239\n",
            "Training Accuracy:  93.75\n",
            "Test Loss:  0.5153320254178623\n",
            "Testing Accuracy:  87.5\n",
            "Epoch:  1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2625/2625 [00:09<00:00, 290.63it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss:  0.42735260559246135\n",
            "Training Accuracy:  93.75\n",
            "Test Loss:  0.47006787534605954\n",
            "Testing Accuracy:  93.75\n",
            "Epoch:  2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2625/2625 [00:09<00:00, 287.22it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss:  0.4015703604421983\n",
            "Training Accuracy:  93.75\n",
            "Test Loss:  0.4495489009072878\n",
            "Testing Accuracy:  93.75\n",
            "Epoch:  3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2625/2625 [00:07<00:00, 371.02it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss:  0.3847395535267798\n",
            "Training Accuracy:  93.75\n",
            "Test Loss:  0.4367110546826517\n",
            "Testing Accuracy:  93.75\n",
            "Epoch:  4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2625/2625 [00:08<00:00, 296.08it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss:  0.3728360932506441\n",
            "Training Accuracy:  93.75\n",
            "Test Loss:  0.4283308763825369\n",
            "Testing Accuracy:  93.75\n",
            "Epoch:  5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2625/2625 [00:08<00:00, 293.08it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss:  0.3642372222288187\n",
            "Training Accuracy:  93.75\n",
            "Test Loss:  0.4229076631935479\n",
            "Testing Accuracy:  93.75\n",
            "Epoch:  6\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2625/2625 [00:07<00:00, 372.23it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss:  0.35772402295782\n",
            "Training Accuracy:  93.75\n",
            "Test Loss:  0.4193020337555101\n",
            "Testing Accuracy:  93.75\n",
            "Epoch:  7\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2625/2625 [00:08<00:00, 327.88it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss:  0.3524687325372719\n",
            "Training Accuracy:  93.75\n",
            "Test Loss:  0.416721130885691\n",
            "Testing Accuracy:  93.75\n",
            "Epoch:  8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2625/2625 [00:09<00:00, 287.56it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss:  0.34801298698839034\n",
            "Training Accuracy:  93.75\n",
            "Test Loss:  0.4148457750012481\n",
            "Testing Accuracy:  93.75\n",
            "Epoch:  9\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2625/2625 [00:08<00:00, 324.52it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss:  0.3438340550225936\n",
            "Training Accuracy:  93.75\n",
            "Test Loss:  0.4131234035152751\n",
            "Testing Accuracy:  93.75\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Y_pred_int = []\n",
        "Y_test_int = []\n",
        "for i in range(Y_pred.shape[1]):\n",
        "  Y_pred_int.append(Y_pred[:,i].argmax())\n",
        "  Y_test_int.append(Y_test[:,i].argmax())"
      ],
      "metadata": {
        "id": "g1ceTL7WCNoi"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.init(project='dl_assgn_1_q_7')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        },
        "id": "Yh4hBNFcBvDk",
        "outputId": "2a92fb73-9770-4592-8a91-53ee3d0301de"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "wandb: Paste an API key from your profile and hit enter:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ··········\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33marunangshudutta218\u001b[0m (\u001b[33marunangshudutta218-iitm\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.7"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250310_031841-reyr6tiu</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/arunangshudutta218-iitm/dl_assgn_1_q_7/runs/reyr6tiu' target=\"_blank\">silver-dust-1</a></strong> to <a href='https://wandb.ai/arunangshudutta218-iitm/dl_assgn_1_q_7' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/arunangshudutta218-iitm/dl_assgn_1_q_7' target=\"_blank\">https://wandb.ai/arunangshudutta218-iitm/dl_assgn_1_q_7</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/arunangshudutta218-iitm/dl_assgn_1_q_7/runs/reyr6tiu' target=\"_blank\">https://wandb.ai/arunangshudutta218-iitm/dl_assgn_1_q_7/runs/reyr6tiu</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/arunangshudutta218-iitm/dl_assgn_1_q_7/runs/reyr6tiu?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
            ],
            "text/plain": [
              "<wandb.sdk.wandb_run.Run at 0x7dde823d5610>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.run.name = \"Confusion_matrix_e_{}_hl_{}_hs_{}_lr_{}_opt_{}_bs_{}_init_{}_ac_{}_l2_{}\".format(epochs, nhl, sz, lr, optimizer, b_sz, weight_init, act_fun, w_d)\n",
        "\n",
        "wandb.log({\n",
        "    \"confusion_matrix\": wandb.plot.confusion_matrix(\n",
        "        y_true=Y_test_int,\n",
        "        preds=Y_pred_int,\n",
        "        class_names=class_names\n",
        "    )\n",
        "})\n",
        "\n",
        "# Finish the W&B run\n",
        "wandb.finish()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "id": "8Kl129THBtQF",
        "outputId": "e687cbf1-6db2-4e40-af67-5984a904f18f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">Confusion_matrix_e_10_hl_3_hs_64_lr_0.0001_opt_adam_bs_16_init_Xavier_ac_tanh_l2_0</strong> at: <a href='https://wandb.ai/arunangshudutta218-iitm/dl_assgn_1_q_7/runs/reyr6tiu' target=\"_blank\">https://wandb.ai/arunangshudutta218-iitm/dl_assgn_1_q_7/runs/reyr6tiu</a><br> View project at: <a href='https://wandb.ai/arunangshudutta218-iitm/dl_assgn_1_q_7' target=\"_blank\">https://wandb.ai/arunangshudutta218-iitm/dl_assgn_1_q_7</a><br>Synced 5 W&B file(s), 1 media file(s), 2 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250310_031841-reyr6tiu/logs</code>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ozSAzwUPD946"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}